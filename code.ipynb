{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing using NLTKÂ¶\n",
    "\n",
    "\n",
    "### About the Data Set:\n",
    "\n",
    "There are 8 different text files of ebooks which are available freely on http://www.gutenberg.org/ . The books are\n",
    "\n",
    "    The Adventures of Tom Sawyer\n",
    "    The Time Machine\n",
    "    The War of the Worlds\n",
    "    Astounding Stories\n",
    "    Common Science\n",
    "    Northanger Abby\n",
    "    General Science\n",
    "    Sailing Alone Around the World\n",
    "\n",
    "### Steps Performed:\n",
    "    -Importing text files\n",
    "    -Text Parsing and transforming operations performed such as conversion to lower case, removal of special characters, contraction words, tokenizing etc.\n",
    "    -Tagging parts of speech to each term\n",
    "    -Stemming terms to get their root word\n",
    "    -Removal of stop words\n",
    "    -Term Document matrix produced with the hightest occuring terms in each document. \n",
    "    -Compared the results of Term document matrix and how POS, Stopwords and Stemming Operation affect the formulated matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Term Document Matrix with POS, Stemming and Stopword Removal\n",
    "    POS = TRUE\n",
    "    \n",
    "    Stemming = True\n",
    "    \n",
    "    Remove Stopwords = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Textfiles/'\n",
    "files = ['T1.txt', 'T2.txt', 'T3.txt', 'T4.txt', 'T5.txt', 'T6.txt','T7.txt', 'T8.txt']\n",
    "term_doc = []\n",
    "pos_tags = True\n",
    "stemming = True\n",
    "remove_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document T1.txt contains a total of 86484  terms.\n",
      "Document T1.txt contains 40039 terms after stemming.\n",
      "\n",
      "\n",
      "Document T2.txt contains a total of 108474  terms.\n",
      "Document T2.txt contains 48289 terms after stemming.\n",
      "\n",
      "\n",
      "Document T3.txt contains a total of 104778  terms.\n",
      "Document T3.txt contains 50177 terms after stemming.\n",
      "\n",
      "\n",
      "Document T4.txt contains a total of 83140  terms.\n",
      "Document T4.txt contains 35269 terms after stemming.\n",
      "\n",
      "\n",
      "Document T5.txt contains a total of 76238  terms.\n",
      "Document T5.txt contains 35030 terms after stemming.\n",
      "\n",
      "\n",
      "Document T6.txt contains a total of 35136  terms.\n",
      "Document T6.txt contains 15670 terms after stemming.\n",
      "\n",
      "\n",
      "Document T7.txt contains a total of 80206  terms.\n",
      "Document T7.txt contains 35461 terms after stemming.\n",
      "\n",
      "\n",
      "Document T8.txt contains a total of 64511  terms.\n",
      "Document T8.txt contains 29797 terms after stemming.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Initialize and Reading File\n",
    "for file in files:\n",
    "    with open (file_path+file, \"r\") as text_file:\n",
    "        adoc = text_file.read()\n",
    "    # Convert to all lower case - required\n",
    "    adoc = (\"%s\" %adoc).lower()\n",
    "\n",
    "    # Replace special characters with spaces\n",
    "    adoc = adoc.replace('-', ' ')\n",
    "    adoc = adoc.replace('_', ' ')\n",
    "    adoc = adoc.replace(',', ' ')\n",
    "\n",
    "    # Replace not contraction with not\n",
    "    adoc = adoc.replace(\"'nt\", \" not\")\n",
    "    adoc = adoc.replace(\"n't\", \" not\")\n",
    "    adoc = adoc.replace(\"'d\", \" \")\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(adoc)\n",
    "    tokens = [word.replace(',', '') for word in tokens]\n",
    "    tokens = [word for word in tokens if ('*' not in word) and word != \"''\" and word !=\"``\"]\n",
    "\n",
    "    for word in tokens:\n",
    "        word = re.sub(r'[^\\w\\d\\s]+','',word)\n",
    "    print(\"\\nDocument \"+file+\" contains a total of\", len(tokens), \" terms.\")\n",
    "    \n",
    "    #POS Tagging\n",
    "    if pos_tags:\n",
    "        tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Remove stop words\n",
    "    if remove_stop:\n",
    "        stop = stopwords.words('english') + list(string.punctuation)\n",
    "        stop.append(\"said\")\n",
    "        # Remove single character words and simple punctuation\n",
    "        tokens = [word for word in tokens if len(word) > 1]\n",
    "        # Remove stop words\n",
    "        if pos_tags:\n",
    "            tokens = [word for word in tokens if word[0] not in stop]\n",
    "            tokens = [word for word in tokens if (not word[0].replace('.','',1).isnumeric()) and word[0]!=\"'s\" ]\n",
    "        else:\n",
    "            tokens = [word for word in tokens if word not in stop]\n",
    "            tokens = [word for word in tokens if word != \"'s\" ]\n",
    "            \n",
    "    # Lemmatization - Stemming with POS\n",
    "    if stemming:\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        wn_tags = {'N':wn.NOUN, 'J':wn.ADJ, 'V':wn.VERB, 'R':wn.ADV}\n",
    "        wnl = WordNetLemmatizer()\n",
    "        stemmed_tokens = []\n",
    "        if pos_tags:\n",
    "            for token in tokens:\n",
    "                term = token[0]\n",
    "                pos = token[1]\n",
    "                pos = pos[0]\n",
    "                try:\n",
    "                    pos = wn_tags[pos]\n",
    "                    stemmed_tokens.append(wnl.lemmatize(term, pos=pos))\n",
    "                except:\n",
    "                    stemmed_tokens.append(stemmer.stem(term))\n",
    "        else:\n",
    "            for token in tokens:\n",
    "                stemmed_tokens.append(stemmer.stem(token))\n",
    "    if stemming:\n",
    "        print(\"Document \"+file+\" contains\", len(stemmed_tokens), \"terms after stemming.\\n\")\n",
    "        tokens = stemmed_tokens\n",
    "        \n",
    "    #Prepare Counts & Add to term_doc\n",
    "\n",
    "    #fdist = FreqDist(word for word in stemmed_tokens)\n",
    "    fdist = FreqDist(tokens)\n",
    "    # Use with Wordnet\n",
    "    td= {}\n",
    "    #term_doc = []\n",
    "    for word, freq in fdist.most_common(2000):\n",
    "        td[word] = freq\n",
    "    term_doc.append(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario: POS= True Remove Stop Words= True  Stemming= True\n",
      "------------------------------------------------------------\n",
      "TERM            TOTAL  D1   D2   D3   D4   D5   D6   D7   D8\n",
      "one             2127  291  437  348  211  312  121  202  205\n",
      "water           2040   47  922  825    7   94    7   55   83\n",
      "make            1928  204  694  262  185  237   63  169  114\n",
      "would           1855  270  407  195  309  222   60  289  103\n",
      "go              1620  212  292   18  239  154  103  374  228\n",
      "come            1511  211  153   62  126  276  155  282  246\n",
      "could           1363  221  121   49  364  195   93  203  117\n",
      "time            1333  137  128  175  167  164  213  216  133\n",
      "see             1188  179  232  129  156  110   72  172  138\n",
      "light           1175   87  461  322   21   92   61   60   71\n",
      "get             1146  171  291   24   76  121   53  315   95\n",
      "air             1126   69  518  412   20   19   23   30   35\n",
      "know            1042  165  102  112  223  119   46  202   73\n",
      "day              939   87   52   82  117  337   49  107  108\n",
      "take             926  129  174   82  110  135   52  178   66\n",
      "upon             891  168   11  163   88   28  113  148  172\n",
      "way              844   78  211  122   73  100   42  116  102\n",
      "thing            832  120  241   10   76   57  100  113  115\n",
      "like             828  173  119   56  100   80   74  130   96\n",
      "man              827  248   38  104   90   90   70   62  125\n",
      "____________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Prepare Term-Document Matrix\n",
    "\n",
    "td_mat = {}\n",
    "for td in term_doc:\n",
    "    td_mat = Counter(td_mat)+Counter(td)\n",
    "td_matrix = {}\n",
    "for k, v in td_mat.items():\n",
    "    td_matrix[k] = [v]\n",
    "\n",
    "for td in term_doc:\n",
    "    for k, v in td_matrix.items():\n",
    "        if k in td:\n",
    "            td_matrix[k].append(td[k])\n",
    "        else:\n",
    "            td_matrix[k].append(0)\n",
    "                \n",
    "#Print Term Document Matrix\n",
    "\n",
    "td_matrix_sorted = sorted(td_matrix.items(), key=operator.itemgetter(1),reverse=True)\n",
    "print(\"Scenario: POS=\", pos_tags, \"Remove Stop Words=\", remove_stop, \" Stemming=\", stemming)\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"TERM            TOTAL  D1   D2   D3   D4   D5   D6   D7   D8\")\n",
    "for i in range(20):\n",
    "    s = '{:<15s}'.format(td_matrix_sorted[i][0])\n",
    "    v = td_matrix_sorted[i][1]\n",
    "    #print(v)\n",
    "    for j in range(9):\n",
    "        s = s + '{:>5d}'.format(v[j])\n",
    "    print('{:<60s}'.format(s))\n",
    "print(\"____________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\kevin\\anaconda3\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from wordcloud) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from wordcloud) (1.16.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Term Document Matrix without POS, Stemming and Stopword Removal\n",
    "    POS = False\n",
    "    \n",
    "    Stemming = False\n",
    "    \n",
    "    Remove Stopwords = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document T1.txt contains a total of 86484  terms.\n",
      "\n",
      "Document T2.txt contains a total of 108474  terms.\n",
      "\n",
      "Document T3.txt contains a total of 104778  terms.\n",
      "\n",
      "Document T4.txt contains a total of 83140  terms.\n",
      "\n",
      "Document T5.txt contains a total of 76238  terms.\n",
      "\n",
      "Document T6.txt contains a total of 35136  terms.\n",
      "\n",
      "Document T7.txt contains a total of 80206  terms.\n",
      "\n",
      "Document T8.txt contains a total of 64511  terms.\n",
      "Scenario: POS= False Remove Stop Words= False  Stemming= False\n",
      "------------------------------------------------------------\n",
      "TERM            TOTAL  D1   D2   D3   D4   D5   D6   D7   D8\n",
      "the            42083 5178 8302 8767 3174 5833 2241 3794 4794\n",
      ".              28176 4818 4935 4228 2793 2803 1763 3832 3004\n",
      "of             19694 2421 3304 4322 2358 2370 1152 1466 2301\n",
      "and            19163 2358 2278 3240 2304 2121 1235 3124 2503\n",
      "a              15432 1968 2772 2719 1536 2092  815 1895 1635\n",
      "to             13276 1864 2056 1941 2239 1583  691 1727 1175\n",
      "in             10300 1118 1848 2206 1265 1370  537  955 1001\n",
      "it              8204 1035 2045  901 1105  703  418 1309  688\n",
      "i               7621  822   51    6 1282 1893 1265 1007 1295\n",
      "that            6840 1165 1115  785  805  722  433 1022  793\n",
      "was             6487 1137  198  118 1112 1335  552 1181  854\n",
      "is              6142  457 2056 2421  532  226  105  187  158\n",
      "not             4576  531  691  506 1044  431  157  958  258\n",
      "as              4470  543  745  900  684  488  261  398  451\n",
      ";               4399  299  979  633 1172  317  113  643  243\n",
      "he              4250 1390  177   39  544  299  122 1252  427\n",
      "you             4234  587 1485   34  918  120  125  884   81\n",
      "with            4226  574  625  525  663  527  215  648  449\n",
      "for             4010  495  436  529  726  726  217  533  348\n",
      "on              3831  366  797  388  411  976  137  378  378\n",
      "____________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "file_path = 'TextFiles/'\n",
    "files = ['T1.txt', 'T2.txt', 'T3.txt', 'T4.txt', 'T5.txt', 'T6.txt','T7.txt', 'T8.txt']\n",
    "term_doc = []\n",
    "pos_tags = False\n",
    "stemming = False\n",
    "remove_stop = False\n",
    "\n",
    "#Initialize and Reading File\n",
    "for file in files:\n",
    "    with open (file_path+file, \"r\") as text_file:\n",
    "        adoc = text_file.read()\n",
    "    # Convert to all lower case - required\n",
    "    adoc = (\"%s\" %adoc).lower()\n",
    "\n",
    "    # Replace special characters with spaces\n",
    "    adoc = adoc.replace('-', ' ')\n",
    "    adoc = adoc.replace('_', ' ')\n",
    "    adoc = adoc.replace(',', ' ')\n",
    "\n",
    "    # Replace not contraction with not\n",
    "    adoc = adoc.replace(\"'nt\", \" not\")\n",
    "    adoc = adoc.replace(\"n't\", \" not\")\n",
    "    adoc = adoc.replace(\"'d\", \" \")\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(adoc)\n",
    "    tokens = [word.replace(',', '') for word in tokens]\n",
    "    tokens = [word for word in tokens if ('*' not in word) and word != \"''\" and word !=\"``\"]\n",
    "\n",
    "    for word in tokens:\n",
    "        word = re.sub(r'[^\\w\\d\\s]+','',word)\n",
    "    print(\"\\nDocument \"+file+\" contains a total of\", len(tokens), \" terms.\")\n",
    "    \n",
    "    #POS Tagging\n",
    "    if pos_tags:\n",
    "        tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Remove stop words\n",
    "    if remove_stop:\n",
    "        stop = stopwords.words('english') + list(string.punctuation)\n",
    "        stop.append(\"said\")\n",
    "        # Remove single character words and simple punctuation\n",
    "        tokens = [word for word in tokens if len(word) > 1]\n",
    "        # Remove stop words\n",
    "        if pos_tags:\n",
    "            tokens = [word for word in tokens if word[0] not in stop]\n",
    "            tokens = [word for word in tokens if (not word[0].replace('.','',1).isnumeric()) and word[0]!=\"'s\" ]\n",
    "        else:\n",
    "            tokens = [word for word in tokens if word not in stop]\n",
    "            tokens = [word for word in tokens if word != \"'s\" ]\n",
    "            \n",
    "    # Lemmatization - Stemming with POS\n",
    "    if stemming:\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        wn_tags = {'N':wn.NOUN, 'J':wn.ADJ, 'V':wn.VERB, 'R':wn.ADV}\n",
    "        wnl = WordNetLemmatizer()\n",
    "        stemmed_tokens = []\n",
    "        if pos_tags:\n",
    "            for token in tokens:\n",
    "                term = token[0]\n",
    "                pos = token[1]\n",
    "                pos = pos[0]\n",
    "                try:\n",
    "                    pos = wn_tags[pos]\n",
    "                    stemmed_tokens.append(wnl.lemmatize(term, pos=pos))\n",
    "                except:\n",
    "                    stemmed_tokens.append(stemmer.stem(term))\n",
    "        else:\n",
    "            for token in tokens:\n",
    "                stemmed_tokens.append(stemmer.stem(token))\n",
    "    if stemming:\n",
    "        print(\"Document \"+file+\" contains\", len(stemmed_tokens), \"terms after stemming.\\n\")\n",
    "        tokens = stemmed_tokens\n",
    "        \n",
    "    #Prepare Counts & Add to term_doc\n",
    "\n",
    "    #fdist = FreqDist(word for word in stemmed_tokens)\n",
    "    fdist = FreqDist(tokens)\n",
    "    # Use with Wordnet\n",
    "    td= {}\n",
    "    #term_doc = []\n",
    "    for word, freq in fdist.most_common(2000):\n",
    "        td[word] = freq\n",
    "    term_doc.append(td)\n",
    "    \n",
    "#Prepare Term-Document Matrix\n",
    "\n",
    "td_mat = {}\n",
    "for td in term_doc:\n",
    "    td_mat = Counter(td_mat)+Counter(td)\n",
    "td_matrix = {}\n",
    "for k, v in td_mat.items():\n",
    "    td_matrix[k] = [v]\n",
    "\n",
    "for td in term_doc:\n",
    "    for k, v in td_matrix.items():\n",
    "        if k in td:\n",
    "            td_matrix[k].append(td[k])\n",
    "        else:\n",
    "            td_matrix[k].append(0)\n",
    "                \n",
    "#Print Term Document Matrix\n",
    "\n",
    "td_matrix_sorted = sorted(td_matrix.items(), key=operator.itemgetter(1),reverse=True)\n",
    "print(\"Scenario: POS=\", pos_tags, \"Remove Stop Words=\", remove_stop, \" Stemming=\", stemming)\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"TERM            TOTAL  D1   D2   D3   D4   D5   D6   D7   D8\")\n",
    "for i in range(20):\n",
    "    s = '{:<15s}'.format(td_matrix_sorted[i][0])\n",
    "    v = td_matrix_sorted[i][1]\n",
    "    #print(v)\n",
    "    for j in range(9):\n",
    "        s = s + '{:>5d}'.format(v[j])\n",
    "    print('{:<60s}'.format(s))\n",
    "print(\"____________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It can be seen that, this matrix doesnt result in a good analysis of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Term Document Matrix with Stemming\n",
    "    POS = Flase\n",
    "    \n",
    "    Stemming = True\n",
    "    \n",
    "    Remove Stopwords = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document T1.txt contains a total of 86484  terms.\n",
      "\n",
      "Document T2.txt contains a total of 108474  terms.\n",
      "\n",
      "Document T3.txt contains a total of 104778  terms.\n",
      "\n",
      "Document T4.txt contains a total of 83140  terms.\n",
      "\n",
      "Document T5.txt contains a total of 76238  terms.\n",
      "\n",
      "Document T6.txt contains a total of 35136  terms.\n",
      "\n",
      "Document T7.txt contains a total of 80206  terms.\n",
      "\n",
      "Document T8.txt contains a total of 64511  terms.\n",
      "Scenario: POS= False Remove Stop Words= True  Stemming= False\n",
      "------------------------------------------------------------\n",
      "TERM            TOTAL  D1   D2   D3   D4   D5   D6   D7   D8\n",
      "one             2061  277  422  340  207  305  117  190  203\n",
      "water           2001   40  920  816    5   79    7   53   81\n",
      "would           1854  270  407  195  309  222   59  289  103\n",
      "could           1363  221  121   49  364  195   93  203  117\n",
      "time            1137  114  100  109  149  152  200  191  122\n",
      "air             1123   69  518  410   19   19   23   30   35\n",
      "light            980   68  407  291   17   76   43   30   48\n",
      "made             922  102  227  145   90  162   36   87   73\n",
      "upon             890  168   11  163   88   28  112  148  172\n",
      "two              823  114  155  132   86  100   28  120   88\n",
      "man              819  248   38  104   90   83   70   61  125\n",
      "tom              814    0    0    0    0    0    0  814    0\n",
      "like             809  168  118   56   92   79   74  127   95\n",
      "little           802   74  139   32  103   88  112  142  112\n",
      "way              797   77  204  109   70   94   38  105  100\n",
      "see              793  112  208   73   90   56   45  139   70\n",
      "much             725   62  204   97  170   66   23   65   38\n",
      "came             720  120   11    7   41  167  105  118  151\n",
      "well             720   58   71  101  171   87   31  175   26\n",
      "back             679  130  176   40   46   53   40  121   73\n",
      "____________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "file_path = 'TextFiles/'\n",
    "files = ['T1.txt', 'T2.txt', 'T3.txt', 'T4.txt', 'T5.txt', 'T6.txt','T7.txt', 'T8.txt']\n",
    "term_doc = []\n",
    "pos_tags = False\n",
    "stemming = False\n",
    "remove_stop = True\n",
    "\n",
    "#Initialize and Reading File\n",
    "for file in files:\n",
    "    with open (file_path+file, \"r\") as text_file:\n",
    "        adoc = text_file.read()\n",
    "    # Convert to all lower case - required\n",
    "    adoc = (\"%s\" %adoc).lower()\n",
    "\n",
    "    # Replace special characters with spaces\n",
    "    adoc = adoc.replace('-', ' ')\n",
    "    adoc = adoc.replace('_', ' ')\n",
    "    adoc = adoc.replace(',', ' ')\n",
    "\n",
    "    # Replace not contraction with not\n",
    "    adoc = adoc.replace(\"'nt\", \" not\")\n",
    "    adoc = adoc.replace(\"n't\", \" not\")\n",
    "    adoc = adoc.replace(\"'d\", \" \")\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(adoc)\n",
    "    tokens = [word.replace(',', '') for word in tokens]\n",
    "    tokens = [word for word in tokens if ('*' not in word) and word != \"''\" and word !=\"``\"]\n",
    "\n",
    "    for word in tokens:\n",
    "        word = re.sub(r'[^\\w\\d\\s]+','',word)\n",
    "    print(\"\\nDocument \"+file+\" contains a total of\", len(tokens), \" terms.\")\n",
    "    \n",
    "    #POS Tagging\n",
    "    if pos_tags:\n",
    "        tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Remove stop words\n",
    "    if remove_stop:\n",
    "        stop = stopwords.words('english') + list(string.punctuation)\n",
    "        stop.append(\"said\")\n",
    "        # Remove single character words and simple punctuation\n",
    "        tokens = [word for word in tokens if len(word) > 1]\n",
    "        # Remove stop words\n",
    "        if pos_tags:\n",
    "            tokens = [word for word in tokens if word[0] not in stop]\n",
    "            tokens = [word for word in tokens if (not word[0].replace('.','',1).isnumeric()) and word[0]!=\"'s\" ]\n",
    "        else:\n",
    "            tokens = [word for word in tokens if word not in stop]\n",
    "            tokens = [word for word in tokens if word != \"'s\" ]\n",
    "            \n",
    "    # Lemmatization - Stemming with POS\n",
    "    if stemming:\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        wn_tags = {'N':wn.NOUN, 'J':wn.ADJ, 'V':wn.VERB, 'R':wn.ADV}\n",
    "        wnl = WordNetLemmatizer()\n",
    "        stemmed_tokens = []\n",
    "        if pos_tags:\n",
    "            for token in tokens:\n",
    "                term = token[0]\n",
    "                pos = token[1]\n",
    "                pos = pos[0]\n",
    "                try:\n",
    "                    pos = wn_tags[pos]\n",
    "                    stemmed_tokens.append(wnl.lemmatize(term, pos=pos))\n",
    "                except:\n",
    "                    stemmed_tokens.append(stemmer.stem(term))\n",
    "        else:\n",
    "            for token in tokens:\n",
    "                stemmed_tokens.append(stemmer.stem(token))\n",
    "    if stemming:\n",
    "        print(\"Document \"+file+\" contains\", len(stemmed_tokens), \"terms after stemming.\\n\")\n",
    "        tokens = stemmed_tokens\n",
    "        \n",
    "    #Prepare Counts & Add to term_doc\n",
    "\n",
    "    #fdist = FreqDist(word for word in stemmed_tokens)\n",
    "    fdist = FreqDist(tokens)\n",
    "    # Use with Wordnet\n",
    "    td= {}\n",
    "    #term_doc = []\n",
    "    for word, freq in fdist.most_common(2000):\n",
    "        td[word] = freq\n",
    "    term_doc.append(td)\n",
    "    \n",
    "#Prepare Term-Document Matrix\n",
    "\n",
    "td_mat = {}\n",
    "for td in term_doc:\n",
    "    td_mat = Counter(td_mat)+Counter(td)\n",
    "td_matrix = {}\n",
    "for k, v in td_mat.items():\n",
    "    td_matrix[k] = [v]\n",
    "\n",
    "for td in term_doc:\n",
    "    for k, v in td_matrix.items():\n",
    "        if k in td:\n",
    "            td_matrix[k].append(td[k])\n",
    "        else:\n",
    "            td_matrix[k].append(0)\n",
    "                \n",
    "#Print Term Document Matrix\n",
    "\n",
    "td_matrix_sorted = sorted(td_matrix.items(), key=operator.itemgetter(1),reverse=True)\n",
    "print(\"Scenario: POS=\", pos_tags, \"Remove Stop Words=\", remove_stop, \" Stemming=\", stemming)\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"TERM            TOTAL  D1   D2   D3   D4   D5   D6   D7   D8\")\n",
    "for i in range(20):\n",
    "    s = '{:<15s}'.format(td_matrix_sorted[i][0])\n",
    "    v = td_matrix_sorted[i][1]\n",
    "    #print(v)\n",
    "    for j in range(9):\n",
    "        s = s + '{:>5d}'.format(v[j])\n",
    "    print('{:<60s}'.format(s))\n",
    "print(\"____________________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Term Document Matrix with Stemming and Stopword Removal\n",
    "    POS = False\n",
    "    \n",
    "    Stemming = True\n",
    "    \n",
    "    Remove Stopwords = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document T1.txt contains a total of 86484  terms.\n",
      "Document T1.txt contains 40081 terms after stemming.\n",
      "\n",
      "\n",
      "Document T2.txt contains a total of 108474  terms.\n",
      "Document T2.txt contains 50486 terms after stemming.\n",
      "\n",
      "\n",
      "Document T3.txt contains a total of 104778  terms.\n",
      "Document T3.txt contains 52753 terms after stemming.\n",
      "\n",
      "\n",
      "Document T4.txt contains a total of 83140  terms.\n",
      "Document T4.txt contains 35293 terms after stemming.\n",
      "\n",
      "\n",
      "Document T5.txt contains a total of 76238  terms.\n",
      "Document T5.txt contains 35184 terms after stemming.\n",
      "\n",
      "\n",
      "Document T6.txt contains a total of 35136  terms.\n",
      "Document T6.txt contains 15669 terms after stemming.\n",
      "\n",
      "\n",
      "Document T7.txt contains a total of 80206  terms.\n",
      "Document T7.txt contains 35425 terms after stemming.\n",
      "\n",
      "\n",
      "Document T8.txt contains a total of 64511  terms.\n",
      "Document T8.txt contains 29810 terms after stemming.\n",
      "\n",
      "Scenario: POS= False Remove Stop Words= True  Stemming= True\n",
      "------------------------------------------------------------\n",
      "TERM            TOTAL  D1   D2   D3   D4   D5   D6   D7   D8\n",
      "one             2130  291  437  348  211  313  123  202  205\n",
      "water           2042   47  922  826    7   95    7   55   83\n",
      "would           1855  270  407  195  309  222   60  289  103\n",
      "could           1363  221  121   49  364  195   93  203  117\n",
      "time            1335  137  129  176  167  164  213  216  133\n",
      "light           1144   86  462  324   21   91   49   50   61\n",
      "air             1139   69  518  412   22   30   23   30   35\n",
      "make            1038  104  475  130   95   81   28   83   42\n",
      "day              940   87   52   82  117  337   49  108  108\n",
      "made             922  102  227  145   90  162   36   87   73\n",
      "upon             891  168   11  163   88   28  113  148  172\n",
      "go               880  100  177    9  164   84   35  213   98\n",
      "see              875  123  218   76  122   68   51  145   72\n",
      "like             874  175  128   58  117   81   75  139  101\n",
      "way              844   78  211  122   73  100   42  116  102\n",
      "man              833  248   38  104   90   92   70   65  126\n",
      "thing            833  120  241   10   76   57  100  114  115\n",
      "two              824  114  155  132   86  100   28  121   88\n",
      "tom              820    0    0    0    0    0    0  820    0\n",
      "look             806  155  110   22  171   49   79  140   80\n",
      "____________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "file_path = 'TextFiles/'\n",
    "files = ['T1.txt', 'T2.txt', 'T3.txt', 'T4.txt', 'T5.txt', 'T6.txt','T7.txt', 'T8.txt']\n",
    "term_doc = []\n",
    "pos_tags = False\n",
    "stemming = True\n",
    "remove_stop = True\n",
    "\n",
    "#Initialize and Reading File\n",
    "for file in files:\n",
    "    with open (file_path+file, \"r\") as text_file:\n",
    "        adoc = text_file.read()\n",
    "    # Convert to all lower case - required\n",
    "    adoc = (\"%s\" %adoc).lower()\n",
    "\n",
    "    # Replace special characters with spaces\n",
    "    adoc = adoc.replace('-', ' ')\n",
    "    adoc = adoc.replace('_', ' ')\n",
    "    adoc = adoc.replace(',', ' ')\n",
    "\n",
    "    # Replace not contraction with not\n",
    "    adoc = adoc.replace(\"'nt\", \" not\")\n",
    "    adoc = adoc.replace(\"n't\", \" not\")\n",
    "    adoc = adoc.replace(\"'d\", \" \")\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(adoc)\n",
    "    tokens = [word.replace(',', '') for word in tokens]\n",
    "    tokens = [word for word in tokens if ('*' not in word) and word != \"''\" and word !=\"``\"]\n",
    "\n",
    "    for word in tokens:\n",
    "        word = re.sub(r'[^\\w\\d\\s]+','',word)\n",
    "    print(\"\\nDocument \"+file+\" contains a total of\", len(tokens), \" terms.\")\n",
    "    \n",
    "    #POS Tagging\n",
    "    if pos_tags:\n",
    "        tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Remove stop words\n",
    "    if remove_stop:\n",
    "        stop = stopwords.words('english') + list(string.punctuation)\n",
    "        stop.append(\"said\")\n",
    "        # Remove single character words and simple punctuation\n",
    "        tokens = [word for word in tokens if len(word) > 1]\n",
    "        # Remove stop words\n",
    "        if pos_tags:\n",
    "            tokens = [word for word in tokens if word[0] not in stop]\n",
    "            tokens = [word for word in tokens if (not word[0].replace('.','',1).isnumeric()) and word[0]!=\"'s\" ]\n",
    "        else:\n",
    "            tokens = [word for word in tokens if word not in stop]\n",
    "            tokens = [word for word in tokens if word != \"'s\" ]\n",
    "            \n",
    "    # Lemmatization - Stemming with POS\n",
    "    if stemming:\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        wn_tags = {'N':wn.NOUN, 'J':wn.ADJ, 'V':wn.VERB, 'R':wn.ADV}\n",
    "        wnl = WordNetLemmatizer()\n",
    "        stemmed_tokens = []\n",
    "        if pos_tags:\n",
    "            for token in tokens:\n",
    "                term = token[0]\n",
    "                pos = token[1]\n",
    "                pos = pos[0]\n",
    "                try:\n",
    "                    pos = wn_tags[pos]\n",
    "                    stemmed_tokens.append(wnl.lemmatize(term, pos=pos))\n",
    "                except:\n",
    "                    stemmed_tokens.append(stemmer.stem(term))\n",
    "        else:\n",
    "            for token in tokens:\n",
    "                stemmed_tokens.append(stemmer.stem(token))\n",
    "    if stemming:\n",
    "        print(\"Document \"+file+\" contains\", len(stemmed_tokens), \"terms after stemming.\\n\")\n",
    "        tokens = stemmed_tokens\n",
    "        \n",
    "    #Prepare Counts & Add to term_doc\n",
    "\n",
    "    #fdist = FreqDist(word for word in stemmed_tokens)\n",
    "    fdist = FreqDist(tokens)\n",
    "    # Use with Wordnet\n",
    "    td= {}\n",
    "    #term_doc = []\n",
    "    for word, freq in fdist.most_common(2000):\n",
    "        td[word] = freq\n",
    "    term_doc.append(td)\n",
    "    \n",
    "#Prepare Term-Document Matrix\n",
    "\n",
    "td_mat = {}\n",
    "for td in term_doc:\n",
    "    td_mat = Counter(td_mat)+Counter(td)\n",
    "td_matrix = {}\n",
    "for k, v in td_mat.items():\n",
    "    td_matrix[k] = [v]\n",
    "\n",
    "for td in term_doc:\n",
    "    for k, v in td_matrix.items():\n",
    "        if k in td:\n",
    "            td_matrix[k].append(td[k])\n",
    "        else:\n",
    "            td_matrix[k].append(0)\n",
    "                \n",
    "#Print Term Document Matrix\n",
    "\n",
    "td_matrix_sorted = sorted(td_matrix.items(), key=operator.itemgetter(1),reverse=True)\n",
    "print(\"Scenario: POS=\", pos_tags, \"Remove Stop Words=\", remove_stop, \" Stemming=\", stemming)\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"TERM            TOTAL  D1   D2   D3   D4   D5   D6   D7   D8\")\n",
    "for i in range(20):\n",
    "    s = '{:<15s}'.format(td_matrix_sorted[i][0])\n",
    "    v = td_matrix_sorted[i][1]\n",
    "    #print(v)\n",
    "    for j in range(9):\n",
    "        s = s + '{:>5d}'.format(v[j])\n",
    "    print('{:<60s}'.format(s))\n",
    "print(\"____________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we know how POS, Stopwords and Stemming affect the Term Document Matrix. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
